1. Deep Learning, Literature, and Aesthetic Meaning

The central proposition of my dissertation is, informally, ‘it’s possible to learn a way of seeing by examining a group of objects that this way of seeing sees the best.’

I take my cue from how compression-based representation-learning neural nets (what AI researches would call ‘autoencoders’ in a broad sense, from RBMs to VAEs and two-way GANs) not only learn a low dimensional representation space, but also a projection from each point in input-space to some point in a low dimensional submanifold within the input space.*

[*This geometrical description draws on the accepted practice of idealizing autoencoders with continuous input spaces and deterministic ‘pixel-independent’ decoders as non-linear dimensionality reduction algorithms. While the core ideas of this talk do carry over to autoencoders with autoregressive decoders, and in part even to pure autoregressive models like GPT architectures, the account becomes more complex.]

Speaking in rough terms, a trained autoencoder’s translation of an input object to its low dimensional representation space — the process of extracting an abstract, quasi-conceptual representation of the object — corresponds to taking the orthogonal projection of the input to a low dimensional submanifold in input-space, then taking the coordinates of the projection in a chart for this submanifold. I argue that there is a useful sense in which a sample from a trained autoencoder’s manifold expresses the autoencoder, and that studying this computational phenomenon gives us a fruitful formal model for thinking about a work of art as the expression of a way of seeing or a worldview.

To get the analogy going, let’s consider four properties that hold for the set of points composing the manifold of a (non-degenerate) trained autoencoder:

It’s the set of all the inputs that the trained net can compressedly encode with zero loss.*
All input reconstructions by the trained net will replace the original input with an element of this set.
The objects in this set collectively exemplify a specific high-fidelity simplification of the objects generated by the distribution underlying the training data.
A neural net trained on this set as its input distribution will very quickly approximate the neural net that generated the set.**
[*The idea that aesthetic artifacts may be unusually compressible goes back to Andrey Kolmogorov and more recently Jürgen Schmidhuber.]

[**Property 4 constitutes an ‘original,’ though elementary, mathematical observation formulated in collaboration with mathematician Tomer Schlank. More precisely, we show that the sample efficiency of the output set is significantly higher than the sample efficiency of the training set.]

Speaking informally, the above properties almost explicitly describe art as we know it: an individual apprehends the world in a particular lossy way that is partly adapted to her formative environment, she produces a mimesis (i.e. interpretive reproduction) of the world that’s simpler than the world in a specific way that bears the mark of her particular lossy apprehension of the world, and other individuals can learn her apprehension of the world by trying to very exactingly apprehend the artefactual world she produced.* I argue that this similarity is not an accident or even an analogy, and properties 1–4 plausibly give something close to a functional definition of an ‘aesthetic symbol:’ a concrete, sensate representation of an abstract worldview. Additionally, properties 1–2 taken together give a near-equivalence between producing a mimesis of the world and curating a selection of minimum-loss objects from the world, as two ways to construct an approximate sample from the image of a net’s projection function. This near-equivalence suggest a way to understand curation, installation, collage, and other Modernist practices that are not immediately mimetic as, nevertheless, concrete sensate representations of a worldview.